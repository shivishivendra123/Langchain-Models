{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Astronomy RAG Model: AI-Powered Knowledge Retrieval** \n",
    "\n",
    "This project aims to develop a **Retrieval-Augmented Generation (RAG) model** trained on an **astronomy book** to provide intelligent and context-aware responses to user queries. By integrating **Large Language Models (LLMs)** with a **vector-based retrieval system**, the model enhances **fact-based** answering by leveraging **retrieved text from the book** rather than relying solely on generative capabilities.  \n",
    "\n",
    "## **ðŸ”¹ Key Features**  \n",
    "- **PDF/Text Loader** â€“ Extracts information from an astronomy book.  \n",
    "- **FAISS Vector Search** â€“ Efficient document retrieval using embeddings.  \n",
    "- **LLM Integration** â€“ Uses GPT or Hugging Face models for intelligent responses.  \n",
    "- **Accurate Scientific Answers** â€“ Provides fact-based insights on black holes, galaxies, space-time, and more.  \n",
    "- **Interactive User Interface** â€“ Command-line or Jupyter Notebook for easy querying.  \n",
    "\n",
    "## **Workflow**  \n",
    "1. **Load** the astronomy book into the system.  \n",
    "2. **Chunk & Embed** the text using **FAISS** and OpenAI/Hugging Face embeddings.  \n",
    "3. **Retrieve** relevant passages based on user queries.  \n",
    "4. **Generate responses** using an LLM, grounded in the retrieved data.  \n",
    "\n",
    "## **Goal**  \n",
    "To create an **AI-powered astronomy assistant** capable of answering **scientific queries** with high accuracy, enhancing research and education in astrophysics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_huggingface import ChatHuggingFace,HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is LangChain? \n",
    "\n",
    "LangChain is a **framework** designed to help developers **build applications** powered by **large language models (LLMs)** like OpenAIâ€™s GPT, Hugging Face models, and other AI models. It provides **modular tools** for integrating language models with **external data sources, retrieval-augmented generation (RAG), memory, and reasoning**.  \n",
    "\n",
    "---\n",
    "\n",
    "## Key Features of LangChain  \n",
    "\n",
    "1. **LLM Integration** â€“ Easily connects with OpenAI, Hugging Face, and other models.  \n",
    "2. **Retrieval-Augmented Generation (RAG)** â€“ Uses **document retrieval** to improve model responses.  \n",
    "3. **Memory** â€“ Stores conversation history for **context-aware** interactions.  \n",
    "4. **Chains** â€“ Sequences multiple steps together (e.g., prompt â†’ LLM â†’ post-processing).  \n",
    "5. **Agents** â€“ Uses AI to decide **which tool** (search, database, API) to call at runtime.  \n",
    "6. **Tools & Plugins** â€“ Integrates with **APIs, databases, embeddings, and vector stores**.  \n",
    "\n",
    "---\n",
    "\n",
    "## Why Use LangChain? \n",
    "**Customizable** â€“ You can fine-tune behavior and data sources.  \n",
    "**Scalable** â€“ Supports small apps and **enterprise-level AI**.  \n",
    "**Multi-Modal** â€“ Works with text, documents, images, and even structured data.  \n",
    "\n",
    "---\n",
    "\n",
    "## Example Use Cases \n",
    "- **Chatbots & Virtual Assistants** \n",
    "- **Legal or Financial Document Analysis**   \n",
    "- **RAG Systems (e.g., PDF Q&A with Vector Search)**   \n",
    "- **Automated Report Generation** \n",
    "- **AI-powered Search Engines**  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing ChatHuggingFace for interating with HuggingFace LLMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **What is Hugging Face?** \n",
    "\n",
    "Hugging Face is an **AI research organization and open-source platform** that provides **pretrained models, datasets, and tools** for **Natural Language Processing (NLP), Computer Vision (CV), and Machine Learning (ML)**.  \n",
    "\n",
    "It is best known for the **Transformers** library, which allows developers to use **state-of-the-art models** like GPT, BERT, Llama, and more.\n",
    "\n",
    "---\n",
    "\n",
    "##  Key Features of Hugging Face \n",
    "\n",
    "1. **Transformers** â€“ A Python library with thousands of **pretrained NLP and CV models**.  \n",
    "2. **Datasets** â€“ A collection of **high-quality datasets** for AI training.  \n",
    "3. **Model Hub** â€“ A cloud-based **repository** to store and share AI models.  \n",
    "4. **Inference API** â€“ Run AI models in the cloud **without setting up infrastructure**.  \n",
    "5. **AutoTrain** â€“ No-code tool for **fine-tuning models** easily.  \n",
    "6. **Accelerate** â€“ Optimized tools for **faster model training** and deployment.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Why Use Hugging Face?**  \n",
    "**Pretrained Models** â€“ Save time with state-of-the-art models.  \n",
    "**Easy Fine-Tuning** â€“ Train models with your **custom data**.  \n",
    "**Cloud & API Support** â€“ Deploy models instantly with APIs.  \n",
    "**Large Community** â€“ Thousands of researchers and developers contribute.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Example Use Cases**  \n",
    "- **Chatbots & Virtual Assistants**  \n",
    "- **Text Summarization**   \n",
    "- **Image Generation (Stable Diffusion)**  \n",
    "- **Speech-to-Text (ASR)** \n",
    "- **Sentiment Analysis**  \n",
    "- **Legal & Financial Document Processing**  \n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import ChatHuggingFace,HuggingFaceEndpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "llm = HuggingFacePipeline.from_model_id(\n",
    "    model_id='TinyLlama/TinyLlama-1.1B-Chat-v1.0',\n",
    "    task='text-generation',\n",
    "    pipeline_kwargs = dict(\n",
    "        temperature = 1,\n",
    "        max_new_tokens = 100\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display,clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_splitter(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=100,chunk_overlap=50)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_document():\n",
    "    loader = PyPDFLoader(\"/Users/shivendragupta/Desktop/Langchain Models/RAG demo/06. Astronomy An overview Autor Poincare.pdf\")\n",
    "    documents = loader.load()\n",
    "    return text_splitter(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_retreiver(docs):\n",
    "    vector_store = FAISS.from_documents(docs , HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2'))\n",
    "\n",
    "    return vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retreived_docs(retreiver,query):\n",
    "    return retreiver.get_relevant_documents(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the Widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41b86327203a4410b91e7c14ecf38a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Generating response...</b>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a710cd7d63f40708b0a0bcfdad1a3fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<b>Generating response...</b>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_input = widgets.Text(placeholder='Enter your query...')\n",
    "submit_button = widgets.Button(description='Search')\n",
    "output_area = widgets.Output()\n",
    "response_box = widgets.Textarea(  # Scrollable output box\n",
    "    value=\"\",\n",
    "    placeholder=\"Response will appear here...\",\n",
    "    layout=widgets.Layout(width=\"100%\", height=\"300px\")  # Adjust height as needed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_prompt(retrived_docs,query):\n",
    "    retre = '\\n'.join([doc.page_content for doc in retrived_docs])\n",
    "    prompt = f\"Based on following text , answer the question : {query} \\n\\n {retre}\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qx/720g_1z55gg_fwcvggbhs3_w0000gn/T/ipykernel_76123/702652017.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  vector_store = FAISS.from_documents(docs , HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2'))\n"
     ]
    }
   ],
   "source": [
    "docs = load_document()\n",
    "reteiver = init_retreiver(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handling the click of submit button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_submit(_):\n",
    "    loading_spinner = widgets.HTML(value=\"<b>Generating response...</b>\")\n",
    "    display(loading_spinner)\n",
    "    retrived_docs = get_retreived_docs(reteiver,query_input.value)\n",
    "    prompt = final_prompt(retrived_docs,query_input.value)\n",
    "    response = chat_model.invoke(prompt) \n",
    "\n",
    "   \n",
    "\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        clean_response = response.content.split(\"</s>\")[-1]\n",
    "        \n",
    "\n",
    "        res = clean_response.split(\"</s>\")[-1]\n",
    "        response_box.value = res\n",
    "        loading_spinner = \"\"\n",
    "        print(\"Generated Output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_button.on_click(handle_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef6e9fc861c4591b533aaf23b598f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', placeholder='Enter your query...'), Button(description='Search', style=ButtonStyâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "loading_spinner = widgets.HTML(value=\"<b>Generating response...</b>\")\n",
    "display(widgets.VBox([query_input, submit_button,output_area,response_box,loading_spinner]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
